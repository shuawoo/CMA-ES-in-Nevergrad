{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Optimizing Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: [ 0.57955723  0.35775149  0.15333719  0.02088996  0.00603453  0.04437182\n",
      "  0.03828975  0.01997754  0.00714532 -0.0034166   0.0196684  -0.03520974\n",
      "  0.02580576  0.02979703 -0.01643259]\n",
      "Best value: 12.791898003495042\n",
      "Runtime: 0.26490187644958496 seconds\n"
     ]
    }
   ],
   "source": [
    "# 用CMA\n",
    "\n",
    "import nevergrad as ng\n",
    "import time\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "# Create optimization problem object\n",
    "optimization_problem = ng.p.Array(shape=(15,))\n",
    "\n",
    "# 使用 CMA\n",
    "optimizer = ng.optimizers.CMA(parametrization=optimization_problem, budget=500, num_workers=1)\n",
    "\n",
    "# 运行优化\n",
    "start_time_conf_split = time.time()\n",
    "recommendation = optimizer.minimize(objective_function)\n",
    "end_time_conf_split = time.time()\n",
    "\n",
    "# 获取最优参数和目标函数值\n",
    "best_params = recommendation.value\n",
    "best_value = objective_function(best_params)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")\n",
    "print(f\"Runtime: {end_time_conf_split - start_time_conf_split} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: [ 0.73870096  0.56155356  0.33570333  0.13202175  0.03478619  0.00403458\n",
      " -0.12580368  0.14569456 -0.02291013  0.05235289  0.05339143  0.09474233\n",
      " -0.04105102  0.05422803  0.00178088]\n",
      "Best value: 16.50967640434098\n",
      "Runtime: 0.6344621181488037 seconds\n"
     ]
    }
   ],
   "source": [
    "# 用ConfSplitOptimizer\n",
    "\n",
    "import nevergrad as ng\n",
    "import time\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "# Create optimization problem object\n",
    "optimization_problem = ng.p.Array(shape=(15,))\n",
    "\n",
    "# 使用 ConfSplitOptimizer，指定3个优化器\n",
    "optimizer = ng.optimizers.ConfSplitOptimizer(num_vars=[5, 5, 5])(parametrization=optimization_problem, budget = 500, num_workers=1)\n",
    "\n",
    "# 运行优化\n",
    "start_time_conf_split = time.time()\n",
    "recommendation = optimizer.minimize(objective_function)\n",
    "end_time_conf_split = time.time()\n",
    "\n",
    "# 获取最优参数和目标函数值\n",
    "best_params = recommendation.value\n",
    "best_value = objective_function(best_params)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")\n",
    "print(f\"Runtime: {end_time_conf_split - start_time_conf_split} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: [ 0.42045405  0.18170958  0.0249857  -0.0116888   0.0350548   0.03245961\n",
      " -0.00872556  0.05552556  0.01210119  0.0917312   0.04964616  0.02816782\n",
      "  0.15638656  0.23446494  0.02673691]\n",
      "Best value: 20.25721648919614\n",
      "Runtime: 0.6936447620391846 seconds\n"
     ]
    }
   ],
   "source": [
    "# 用ConfSplitOptimizer\n",
    "\n",
    "import nevergrad as ng\n",
    "import time\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "# Create optimization problem object\n",
    "optimization_problem = ng.p.Array(shape=(15,))\n",
    "\n",
    "# 使用 ConfSplitOptimizer，指定3个优化器\n",
    "optimizer = ng.optimizers.ConfSplitOptimizer(num_vars=[2, 8, 5])(parametrization=optimization_problem, budget=500, num_workers=1)\n",
    "\n",
    "# 运行优化\n",
    "start_time_conf_split = time.time()\n",
    "recommendation = optimizer.minimize(objective_function)\n",
    "end_time_conf_split = time.time()\n",
    "\n",
    "# 获取最优参数和目标函数值\n",
    "best_params = recommendation.value\n",
    "best_value = objective_function(best_params)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")\n",
    "print(f\"Runtime: {end_time_conf_split - start_time_conf_split} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMA Results:\n",
      "Best parameters: [ 0.16257693  0.02057829  0.01203292  0.01608909 -0.00051406 -0.01626659\n",
      " -0.01394242]\n",
      "Best value: 5.7275605378120495\n",
      "Runtime: 0.28734421730041504 seconds\n",
      "\n",
      "ConfSplitOptimizer Results:\n",
      "Best parameters: [ 8.70441694e-01  7.98676597e-01  7.15185418e-01  5.26360516e-01\n",
      "  3.46477318e-01  1.12097794e-01 -8.28874231e-04]\n",
      "Best value: 2.8722563223310864\n",
      "Runtime: 0.6560161113739014 seconds\n"
     ]
    }
   ],
   "source": [
    "# compare runtime\n",
    "import nevergrad as ng\n",
    "import time\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "# Create optimization problem object\n",
    "optimization_problem = ng.p.Array(shape=(7,))\n",
    "\n",
    "# 使用 CMA 运行优化，并记录运行时间\n",
    "start_time_cma = time.time()\n",
    "optimizer_cma = ng.optimizers.CMA(parametrization=optimization_problem, budget=500, num_workers=1)\n",
    "recommendation_cma = optimizer_cma.minimize(objective_function)\n",
    "end_time_cma = time.time()\n",
    "\n",
    "# 使用 ConfSplitOptimizer 运行优化，并记录运行时间\n",
    "start_time_conf_split = time.time()\n",
    "optimizer_conf_split = ng.optimizers.ConfSplitOptimizer(num_vars=[2, 2, 3])(parametrization=optimization_problem, budget=500, num_workers=1)\n",
    "recommendation_conf_split = optimizer_conf_split.minimize(objective_function)\n",
    "end_time_conf_split = time.time()\n",
    "\n",
    "# 获取最优参数和目标函数值\n",
    "best_params_cma = recommendation_cma.value\n",
    "best_value_cma = objective_function(best_params_cma)\n",
    "\n",
    "best_params_conf_split = recommendation_conf_split.value\n",
    "best_value_conf_split = objective_function(best_params_conf_split)\n",
    "\n",
    "# 打印结果和运行时间\n",
    "print(\"CMA Results:\")\n",
    "print(f\"Best parameters: {best_params_cma}\")\n",
    "print(f\"Best value: {best_value_cma}\")\n",
    "print(f\"Runtime: {end_time_cma - start_time_cma} seconds\\n\")\n",
    "\n",
    "print(\"ConfSplitOptimizer Results:\")\n",
    "print(f\"Best parameters: {best_params_conf_split}\")\n",
    "print(f\"Best value: {best_value_conf_split}\")\n",
    "print(f\"Runtime: {end_time_conf_split - start_time_conf_split} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "\n",
    "import nevergrad as ng\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "optimization_problem = ng.p.Array(shape=(5,))\n",
    "\n",
    "# 设置 early_stopping 参数\n",
    "optimizer = ng.optimizers.CMA(parametrization=optimization_problem, budget=100, num_workers=1, early_stopping=50)\n",
    "\n",
    "# 运行优化\n",
    "recommendation = optimizer.minimize(objective_function)\n",
    "\n",
    "# 获取最优参数和目标函数值\n",
    "best_params = recommendation.value\n",
    "best_value = objective_function(best_params)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "Current recommendation: [0. 0. 0. 0. 0.]\n",
      "Current loss: 4.0\n",
      "\n",
      "Current iteration: 1\n",
      "Current recommendation: [-0.01299618  0.02345659 -0.28010096 -0.85122244  0.02580899]\n",
      "Current loss: 150.23456567817664\n",
      "\n",
      "Current iteration: 2\n",
      "Current recommendation: [-0.01299618  0.02345659 -0.28010096 -0.85122244  0.02580899]\n",
      "Current loss: 150.23456567817664\n",
      "\n",
      "Current iteration: 3\n",
      "Current recommendation: [ 0.56570177 -0.17551679 -0.17683736 -0.67594233  0.67327543]\n",
      "Current loss: 89.32833762930835\n",
      "\n",
      "Current iteration: 4\n",
      "Current recommendation: [ 0.56570177 -0.17551679 -0.17683736 -0.67594233  0.67327543]\n",
      "Current loss: 89.32833762930835\n",
      "\n",
      "Current iteration: 5\n",
      "Current recommendation: [-0.08561625  0.20875121  0.37121994 -0.20618007 -0.62178478]\n",
      "Current loss: 74.4082074925277\n",
      "\n",
      "Current iteration: 6\n",
      "Current recommendation: [-0.08561625  0.20875121  0.37121994 -0.20618007 -0.62178478]\n",
      "Current loss: 74.4082074925277\n",
      "\n",
      "Current iteration: 7\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 8\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 9\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 10\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 11\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 12\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 13\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 14\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 15\n",
      "Current recommendation: [-0.47157624 -0.25724431 -0.27584651 -0.28757524  0.1933456 ]\n",
      "Current loss: 56.183616521135534\n",
      "\n",
      "Current iteration: 16\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 17\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 18\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 19\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 20\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 21\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 22\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 23\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 24\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 25\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 26\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 27\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 28\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 29\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 30\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 31\n",
      "Current recommendation: [-0.091221   -0.27261304  0.17177455 -0.35346874 -0.35143062]\n",
      "Current loss: 51.530244342513434\n",
      "\n",
      "Current iteration: 32\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 33\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 34\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 35\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 36\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 37\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 38\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 39\n",
      "Current recommendation: [-0.18474316  0.14579397 -0.49681921 -0.03827739  0.10249594]\n",
      "Current loss: 42.68814181774519\n",
      "\n",
      "Current iteration: 40\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 41\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 42\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 43\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 44\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 45\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 46\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 47\n",
      "Current recommendation: [-0.18036068  0.04756396 -0.29632967  0.17520815 -0.18269305]\n",
      "Current loss: 18.91684921492323\n",
      "\n",
      "Current iteration: 48\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 49\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 50\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 51\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 52\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 53\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 54\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 55\n",
      "Current recommendation: [-0.05623499  0.05697546 -0.01035006  0.15090082  0.03217555]\n",
      "Current loss: 6.3374933050085485\n",
      "\n",
      "Current iteration: 56\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 57\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 58\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 59\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 60\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 61\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 62\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 63\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 64\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 65\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 66\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 67\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 68\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 69\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 70\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 71\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 72\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 73\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 74\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 75\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 76\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 77\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 78\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 79\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 80\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 81\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 82\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 83\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 84\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 85\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 86\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 87\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 88\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 89\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 90\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 91\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 92\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 93\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 94\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 95\n",
      "Current recommendation: [ 0.14885577  0.11526601 -0.08176831  0.07258628 -0.03901432]\n",
      "Current loss: 5.9383491311559125\n",
      "\n",
      "Current iteration: 96\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 97\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 98\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 99\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 100\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 101\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 102\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 103\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 104\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 105\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 106\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 107\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 108\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 109\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 110\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 111\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 112\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 113\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 114\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 115\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 116\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 117\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 118\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 119\n",
      "Current recommendation: [ 0.13891521 -0.01889531 -0.03394949  0.02206249 -0.05530092]\n",
      "Current loss: 4.423539256905271\n",
      "\n",
      "Current iteration: 120\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 121\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 122\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 123\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 124\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 125\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 126\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 127\n",
      "Current recommendation: [ 0.12907169  0.00109569 -0.05158185  0.06062409  0.02425711]\n",
      "Current loss: 4.413218479556157\n",
      "\n",
      "Current iteration: 128\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 129\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 130\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 131\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 132\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 133\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 134\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 135\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 136\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 137\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 138\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 139\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 140\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 141\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 142\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 143\n",
      "Current recommendation: [ 0.0332638   0.01462529  0.05281547  0.03024062 -0.03056401]\n",
      "Current loss: 4.212548479891275\n",
      "\n",
      "Current iteration: 144\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 145\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 146\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 147\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 148\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 149\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 150\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 151\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 152\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 153\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 154\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 155\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 156\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 157\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 158\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 159\n",
      "Current recommendation: [ 0.13677913 -0.00479476  0.05320245  0.04449425  0.01238307]\n",
      "Current loss: 4.086635267605447\n",
      "\n",
      "Current iteration: 160\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 161\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 162\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 163\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 164\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 165\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 166\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 167\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 168\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 169\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 170\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 171\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 172\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 173\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 174\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 175\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 176\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 177\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 178\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 179\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 180\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 181\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 182\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 183\n",
      "Current recommendation: [ 0.1160962   0.02512781  0.01684941  0.02185624 -0.00751915]\n",
      "Current loss: 3.7478169491262125\n",
      "\n",
      "Current iteration: 184\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 185\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 186\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 187\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 188\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 189\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 190\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 191\n",
      "Current recommendation: [0.1470983  0.02770766 0.00373985 0.02550239 0.00229038]\n",
      "Current loss: 3.6847759815697274\n",
      "\n",
      "Current iteration: 192\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 193\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 194\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 195\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 196\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 197\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 198\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 199\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 200\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 201\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 202\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 203\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 204\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 205\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 206\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 207\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 208\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 209\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 210\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 211\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 212\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 213\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 214\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 215\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 216\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 217\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 218\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 219\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 220\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 221\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 222\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 223\n",
      "Current recommendation: [ 0.17856734  0.02448413 -0.0086057   0.0099221   0.00596976]\n",
      "Current loss: 3.651020864369861\n",
      "\n",
      "Current iteration: 224\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 225\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 226\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 227\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 228\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 229\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 230\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 231\n",
      "Current recommendation: [ 0.16865421  0.030334    0.02026949  0.00940504 -0.00937961]\n",
      "Current loss: 3.627389006664368\n",
      "\n",
      "Current iteration: 232\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 233\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 234\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 235\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 236\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 237\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 238\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 239\n",
      "Current recommendation: [ 0.19360231  0.02995107  0.0037522   0.00723222 -0.00656034]\n",
      "Current loss: 3.5854390258213282\n",
      "\n",
      "Current iteration: 240\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 241\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 242\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 243\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 244\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 245\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 246\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 247\n",
      "Current recommendation: [0.20301161 0.03629661 0.01484522 0.01450239 0.01261073]\n",
      "Current loss: 3.562142652393777\n",
      "\n",
      "Current iteration: 248\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 249\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 250\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 251\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 252\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 253\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 254\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 255\n",
      "Current recommendation: [0.26027734 0.04440459 0.01765174 0.01876381 0.00104352]\n",
      "Current loss: 3.5013413915637726\n",
      "\n",
      "Current iteration: 256\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 257\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 258\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 259\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 260\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 261\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 262\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 263\n",
      "Current recommendation: [ 0.22231706  0.06349998  0.00719875  0.01674795 -0.00542158]\n",
      "Current loss: 3.4862029260333625\n",
      "\n",
      "Current iteration: 264\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 265\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 266\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 267\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 268\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 269\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 270\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 271\n",
      "Current recommendation: [0.25198151 0.07952769 0.02018432 0.00539789 0.01529686]\n",
      "Current loss: 3.4267886870567663\n",
      "\n",
      "Current iteration: 272\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 273\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 274\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 275\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 276\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 277\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 278\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 279\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 280\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 281\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 282\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 283\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 284\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 285\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 286\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 287\n",
      "Current recommendation: [0.26150588 0.07202875 0.03272094 0.0152325  0.00866215]\n",
      "Current loss: 3.4161954365847005\n",
      "\n",
      "Current iteration: 288\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 289\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 290\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 291\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 292\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 293\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 294\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 295\n",
      "Current recommendation: [0.33518918 0.1024081  0.00755827 0.01648546 0.01234071]\n",
      "Current loss: 3.2521867144268963\n",
      "\n",
      "Current iteration: 296\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 297\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 298\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 299\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 300\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 301\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 302\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 303\n",
      "Current recommendation: [0.33599503 0.11780101 0.02703846 0.01865011 0.02265814]\n",
      "Current loss: 3.230494988522724\n",
      "\n",
      "Current iteration: 304\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 305\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 306\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 307\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 308\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 309\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 310\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 311\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 312\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 313\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 314\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 315\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 316\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 317\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 318\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 319\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 320\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 321\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 322\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 323\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 324\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 325\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 326\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 327\n",
      "Current recommendation: [0.36702737 0.12430148 0.02471713 0.01479576 0.01960778]\n",
      "Current loss: 3.1664382716445076\n",
      "\n",
      "Current iteration: 328\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 329\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 330\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 331\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 332\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 333\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 334\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 335\n",
      "Current recommendation: [0.37022387 0.13412608 0.02646077 0.00529412 0.01154422]\n",
      "Current loss: 3.1069866292074106\n",
      "\n",
      "Current iteration: 336\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 337\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 338\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 339\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 340\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 341\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Current iteration: 342\n",
      "Current recommendation: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Current loss: 3.0254632402242723\n",
      "\n",
      "Best parameters: [ 0.39331293  0.15328581  0.02529368  0.00792402 -0.00187202]\n",
      "Best value: 3.0254632402242723\n"
     ]
    }
   ],
   "source": [
    "# early stopping example\n",
    "\n",
    "import nevergrad as ng\n",
    "\n",
    "# Objective function (example: Rosenbrock function)\n",
    "def objective_function(x):\n",
    "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "# Create optimization problem object\n",
    "optimization_problem = ng.p.Array(shape=(5,))\n",
    "\n",
    "# Create CMA optimizer\n",
    "optimizer = ng.optimizers.CMA(parametrization=optimization_problem, budget=500)\n",
    "\n",
    "# Define early stopping condition (loss < 12 and loss is not None)\n",
    "early_stopping = ng.callbacks.EarlyStopping(lambda opt: opt.current_bests[\"minimum\"].mean is not None and opt.current_bests[\"minimum\"].mean < 3)\n",
    "\n",
    "# Custom callback function to print information\n",
    "def print_info(opt):\n",
    "    current_recommendation = opt.recommend()\n",
    "    print(f\"Current iteration: {opt.num_ask}\")\n",
    "    print(f\"Current recommendation: {current_recommendation.value}\")\n",
    "    print(f\"Current loss: {objective_function(current_recommendation.value)}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Register custom callback\n",
    "optimizer.register_callback(\"ask\", early_stopping)\n",
    "optimizer.register_callback(\"ask\", print_info)\n",
    "\n",
    "# Run optimization\n",
    "recommendation = optimizer.minimize(objective_function)\n",
    "\n",
    "# Get the best parameters and the best value\n",
    "best_params = recommendation.value\n",
    "best_value = objective_function(best_params)\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value: {best_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
